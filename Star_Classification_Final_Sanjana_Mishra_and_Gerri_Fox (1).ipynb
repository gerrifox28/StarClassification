{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfQJfYnPIkUv"
   },
   "source": [
    "## Star Classification - Comparing ML Methods K-Nearest Neighbors, AdaBoost and Logistic Regression\n",
    "### Sanjana Mishra and Gerri Fox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "i6gb8b7hFZFA"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score, f1_score\n",
    "from sklearn.ensemble import AdaBoostClassifier \n",
    "from sklearn import tree \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import utils \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YesO1u6mCUng"
   },
   "source": [
    "### Our Data:\n",
    "\n",
    "The [Kaggle Dataset](https://www.kaggle.com/brsdincer/star-type-classification) we used.\n",
    "\n",
    "**Our data is provided from NASA via Kaggle and includes the following features collected about stars:**\n",
    "\n",
    "Temperature (Kelvin)\n",
    "\n",
    "Luminosity (L/Lo*)\n",
    "\n",
    "Radius (R/Ro**)\n",
    "\n",
    "Absolute Magnitude (Mv)\n",
    "\n",
    "General Color of Spectrum (Red, Blue, etc.)\n",
    "\n",
    "Spectral Class (O,B,A,F,G,K,M)\n",
    "\n",
    "**These features can then be used to predict the type of each provided star, with the types being the following:**\n",
    "\n",
    "Red Dwarf (0)\n",
    "\n",
    "Brown Dwarf (1)\n",
    "\n",
    "White Dwarf (2)\n",
    "\n",
    "Main Sequence (3)\n",
    "\n",
    "Super Giants (4)\n",
    "\n",
    "Hyper Giants (5)\n",
    "\n",
    "\n",
    "*Lo = 3.828 x 10^26 Watts (Avg Luminosity of Sun)\n",
    "\n",
    "**Ro = 6.9551 x 10^8 m (Avg Radius of Sun)\n",
    "\n",
    "\n",
    "**The types of each star were already converted to integers in our dataset, but the color and spectral class were not. So, to get the data you see below, we first conducted some feature engineering to convert both of these fields to numerical data as follows:**\n",
    "\n",
    "Blue (1)\n",
    "\n",
    "Blue White (2)\n",
    "\n",
    "Orange (3)\n",
    "\n",
    "Orange-Red (4)\n",
    "\n",
    "Pale yellow orange (5)\n",
    "\n",
    "Red (6)\n",
    "\n",
    "White (7)\n",
    "\n",
    "White-yellow (8)\n",
    "\n",
    "Whitish (9)\n",
    "\n",
    "Yellow white (10)\n",
    "\n",
    "Yellowish (11)\n",
    "\n",
    "and \n",
    "\n",
    "O (1)\n",
    "\n",
    "B (2)\n",
    "\n",
    "A (3)\n",
    "\n",
    "F (4)\n",
    "\n",
    "G (5)\n",
    "\n",
    "K (6)\n",
    "\n",
    "M (7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "id": "bhHtou-kC13j",
    "outputId": "b7573eb6-9481-433e-b1ff-ab7a6f3fdd70"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperature</th>\n",
       "      <th>L</th>\n",
       "      <th>R</th>\n",
       "      <th>A_M</th>\n",
       "      <th>Color</th>\n",
       "      <th>Spectral_Class</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33750</td>\n",
       "      <td>220000</td>\n",
       "      <td>26</td>\n",
       "      <td>-6.1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19860</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.0131</td>\n",
       "      <td>11.34</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33300</td>\n",
       "      <td>240000</td>\n",
       "      <td>12</td>\n",
       "      <td>-6.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21020</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0112</td>\n",
       "      <td>11.52</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18290</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.00934</td>\n",
       "      <td>12.78</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0 Temperature       L        R    A_M Color Spectral_Class Type\n",
       "1       33750  220000       26   -6.1     1              2    4\n",
       "2       19860  0.0011   0.0131  11.34     1              2    2\n",
       "3       33300  240000       12   -6.5     1              2    4\n",
       "4       21020  0.0015   0.0112  11.52     1              2    2\n",
       "5       18290  0.0013  0.00934  12.78     1              2    2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('StarsEdited.csv', header=None) # read in the data into a dataframe\n",
    "df = df.dropna() # drop any non existent data \n",
    "# https://www.codegrepper.com/code-examples/python/how+to+make+1st+row+as+header+in+pandas \n",
    "new_header = df.iloc[0] #grab the first row for the header\n",
    "df = df[1:] #take the data less the header row\n",
    "df.columns = new_header #set the header row as the df header\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WttD9a4fGslT"
   },
   "source": [
    "## First, we chose to implement **K-Nearest Neighbors** from scratch:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4djqVQ6pFZFC"
   },
   "outputs": [],
   "source": [
    "class KNN:\n",
    "    ''' A class that has the necessary methods to train a machine learning\n",
    "        model following K-Nearest Neighbors.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, k):\n",
    "        ''' Constructor for KNN.\n",
    "\n",
    "            Args:\n",
    "              k (int): represents the number of neighbors to consider\n",
    "        '''\n",
    "        self.k = k\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        ''' Fits this object using the given data.\n",
    "\n",
    "          Args:\n",
    "              X (np.array): Represents the X training data\n",
    "              y (np.array): Represents the y training data \n",
    "        '''\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        \n",
    "    #https://www.geeksforgeeks.org/calculate-the-euclidean-distance-using-numpy/\n",
    "    def distance(self, X1, X2):\n",
    "        ''' Calculates the euclidean distance between two given points.\n",
    "\n",
    "            Args:\n",
    "              X1 (int): the first point to get the distance from \n",
    "              X2 (int): the second point to get the distance to\n",
    "\n",
    "            Returns: \n",
    "              distance (int): the distance between the two given points \n",
    "        '''\n",
    "        distance = np.sum(np.square(X1 - X2))\n",
    "        return np.sqrt(distance)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        ''' Predicts the Y data from the given X data using the trained model.\n",
    "\n",
    "            Args:\n",
    "              X_test (np.array): the X data to classify\n",
    "\n",
    "            Returns:\n",
    "              final_output (np.array): the corresponding Y data classifying\n",
    "                                       the given X data \n",
    "        '''\n",
    "        \n",
    "        final_output = [] # initialize final output array\n",
    "\n",
    "        # go through all given x values\n",
    "        for i in range(len(X_test)):\n",
    "            d = [] # initialize array to keep track of distances \n",
    "            votes = [] # initialize array to keep track of votes \n",
    "\n",
    "            # go through all x training data that the model is fitted with\n",
    "            for j in range(len(X_train)):\n",
    "                # calculate the distance between all the training points \n",
    "                dist = self.distance(X_train[j] , X_test[i])\n",
    "                # append the distance \n",
    "                d.append([dist, j])\n",
    "          \n",
    "            d.sort() # sort the distances in ascending order \n",
    "            d = d[0:self.k] # take the first k nearest neigbors \n",
    "\n",
    "            # for all distances and indices left in the distances array\n",
    "            for d, j in d:\n",
    "                # append a vote for the corresponding y value\n",
    "                votes.append(float(y_train.iloc[j]))\n",
    "\n",
    "            # determine the y value with the most votes \n",
    "            ans = Counter(votes).most_common(1)[0][0]\n",
    "            # append our final classification for this value to the array \n",
    "            final_output.append(ans)\n",
    "            \n",
    "        return final_output\n",
    "    \n",
    "    def score(self, X_test, y_test):\n",
    "        ''' Determines the accuracy of using the trained model to predict the \n",
    "            classifications of the given X values against the given true y values.\n",
    "\n",
    "          Args:\n",
    "              X_test (np.array): the x values to predict\n",
    "              y_test (np.array): the actual classifications of the given x values\n",
    "\n",
    "          Returns: \n",
    "              accuracy (int): the percentage of correct classifications \n",
    "        '''\n",
    "        predictions = self.predict(X_test) # get the y predictions\n",
    "\n",
    "        # calculate the accuracy \n",
    "        return (predictions == y_test).sum() / len(y_test) \n",
    "    \n",
    "#https://medium.com/analytics-vidhya/implementing-k-nearest-neighbours-knn-without-using-scikit-learn-3905b4decc3c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0TRugc_H2uO"
   },
   "source": [
    "### A function made to perform cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "AYRXj9vLFZFE"
   },
   "outputs": [],
   "source": [
    "def cv_train(x, y_true):\n",
    "    \"\"\" leave one out cross validation regression of x, y using KNN\n",
    "    \n",
    "    Args:\n",
    "        x (np.array): input features (star classification features)\n",
    "        y_true (np.array): output features (types of stars)\n",
    "        \n",
    "    Returns:\n",
    "        y_pred (np.array): cross validated y predictions based on given data\n",
    "    \"\"\"\n",
    "    # initialize kfold\n",
    "    n_splits = 10\n",
    "    kfold = KFold(n_splits=n_splits)\n",
    "    \n",
    "    # initialize knn\n",
    "    clf = KNN(3)\n",
    "    \n",
    "    y_pred = np.empty_like(y_true)\n",
    "    for train_idx, test_idx in kfold.split(x):\n",
    "        # split data\n",
    "        x_train = x[train_idx, :]\n",
    "        y_train = y_true.iloc[train_idx]\n",
    "        x_test = x[test_idx, :]\n",
    "        \n",
    "        # fit classifier\n",
    "        clf.fit(x_train, y_train)\n",
    "        \n",
    "        # predict\n",
    "        y_pred[test_idx] = clf.predict(x_test)\n",
    "    \n",
    "    return y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chLM51skIJba"
   },
   "source": [
    "### Evaluating the accuracy, error and R2 score of KNN on classifying our dataset: \n",
    "\n",
    "*All accuracies/comparison metrics etc. can always be found printed below the code box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vQeftRUnFZFF",
    "outputId": "f62b6970-1ca5-41e8-82c5-7f7c29aff19b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.95\n",
      "Error:  0.050000000000000044\n",
      "R2 score: 0.9904761904761905\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "outcome = df[df.columns[-1]] # get the last column which represents the types\n",
    "features = scaler.fit_transform(df.drop(df.columns[-1], axis=1))\n",
    "\n",
    "# apply pca on data\n",
    "pca = PCA(whiten=True)\n",
    "features_pca = pca.fit_transform(features)\n",
    "\n",
    "# split data into training and testing data \n",
    "X_train, X_test, y_train, y_test = train_test_split(features_pca, outcome, test_size = 0.25, stratify=outcome)\n",
    "\n",
    "# construct KNN classifier \n",
    "clf = KNN(5)\n",
    "# fit the classifer with training data\n",
    "clf.fit(X_train, y_train) \n",
    "# use the training data to predict the test\n",
    "prediction = clf.predict(X_test)\n",
    "# calculate the accuracy of this classifier \n",
    "score = clf.score(X_test, np.array(y_test).astype(float))\n",
    "print(f'Accuracy Score: {score}')\n",
    "print('Error: ', (1-score))\n",
    "\n",
    "# apply cross validation \n",
    "y_pred_cross = cv_train(X_train, y_train)\n",
    "# compute r2 score \n",
    "r2 = r2_score(y_true=y_train, y_pred=y_pred_cross)\n",
    "print(f'R2 score: {r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then we implemented the KNN model using sklearn to compare to our own implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "eT_chypE77tb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy of sklearn KNN: 0.95\n"
     ]
    }
   ],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors = 5) # initialize sklearn knn model\n",
    "\n",
    "clf.fit(X_train, y_train) # fit the sklearn knn \n",
    "\n",
    "predictions = clf.predict(X_test) # predict using sklearn knn model\n",
    "\n",
    "score = clf.score(X_test, y_test) # get the accuracy of the sklearn knn model\n",
    "\n",
    "print(f' Accuracy of sklearn KNN: {score}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_6kUty1IIBJ"
   },
   "source": [
    "## The next ML model we chose to implement using sckit learn to compare is **AdaBoost**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6baRznIfFZFG",
    "outputId": "f4bb71c1-30fe-4e9b-eb4c-0ee0c32ef033"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AB Training set values for 1 value: \n",
      " Error:  0.0 \n",
      " F1 Score:  1.0 \n",
      "\n",
      "AB Testing Set values for 1 value: \n",
      " Error:  0.050000000000000044 \n",
      " F1 Score:  0.9509704212221376 \n",
      "\n",
      "AB Training set values for 5 value: \n",
      " Error:  0.0 \n",
      " F1 Score:  1.0 \n",
      "\n",
      "AB Testing Set values for 5 value: \n",
      " Error:  0.050000000000000044 \n",
      " F1 Score:  0.9509704212221376 \n",
      "\n",
      "AB Training set values for 10 value: \n",
      " Error:  0.0 \n",
      " F1 Score:  1.0 \n",
      "\n",
      "AB Testing Set values for 10 value: \n",
      " Error:  0.06666666666666665 \n",
      " F1 Score:  0.934470013417382 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dec_tree = tree.DecisionTreeClassifier() # initialize Decision tree classifier\n",
    "\n",
    "b_classifier = [1, 5, 10] # represents estimators we are testing \n",
    "\n",
    "# go through each estimator \n",
    "for classifier in b_classifier:\n",
    "    dec_tree = tree.DecisionTreeClassifier() # initialize Decision tree classifier\n",
    "    # initialize AdaBoost classifier using the current estimator and the \n",
    "    # decision tree and fit it using the x and y training data from above\n",
    "    adaboost = AdaBoostClassifier(n_estimators = classifier, base_estimator = dec_tree).fit(X_train, y_train)\n",
    "    adaboost_pred_train = adaboost.predict(X_train) # predict with training data\n",
    "    accuracy = accuracy_score(y_train, adaboost_pred_train) # get the accuracy of using the training data\n",
    "    f1 = f1_score(y_train, adaboost_pred_train, average = 'macro') # get the f1 score of using the training data\n",
    "    #auc = roc_auc_score(y_train, adaboost_pred_train, average = None)\n",
    "    print(\"AB Training set values for\", classifier,\"value: \\n\", \n",
    "          \"Error: \", (1 - accuracy), \"\\n\",\n",
    "          \"F1 Score: \", f1 , \"\\n\") \n",
    "          #\"AUC: \", auc, \"\\n\")\n",
    "         \n",
    "    adaboost_pred_test = adaboost.predict(X_test) # predict with testing data\n",
    "    accuracy = accuracy_score(y_test, adaboost_pred_test) # get the accuracy of using the testing data\n",
    "    f1 = f1_score(y_test, adaboost_pred_test, average = \"macro\")  # get the f1 score of using the training data\n",
    "    #auc = roc_auc_score(y_test, adaboost_pred_test)\n",
    "    print(\"AB Testing Set values for\", classifier,\"value: \\n\", \n",
    "          \"Error: \", (1 - accuracy), \"\\n\",\n",
    "          \"F1 Score: \",f1, \"\\n\") \n",
    "          #\"AUC: \", \"{:.12f}\".format(auc), \"\\n _____________________\\n\") \n",
    "         \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfWGgqbDIXKr"
   },
   "source": [
    "## Lastly, we chose to implement **Logistic Regression** from scikit learn as our last ML model for our comparison: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "fSBPqVFDFZFI",
    "outputId": "7fd4c75e-8bff-4c9c-c399-aca79e097ed5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: \n",
      "Training Data: \n",
      " Accuracy:  0.9888888888888889 \n",
      " Error:  0.011111111111111072 \n",
      " Precision:  0.989068100358423 \n",
      " Recall:  0.9888888888888889 \n",
      "\n",
      "Testing Data: \n",
      " Accuracy:  0.9666666666666667 \n",
      " Error:  0.033333333333333326 \n",
      " Precision:  0.9722222222222223 \n",
      " Recall:  0.9666666666666667 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fit a Logistic Regression model using the above x and y training data \n",
    "log = LogisticRegression(max_iter=10000).fit(X_train, y_train)  \n",
    "print (\"Logistic Regression: \")\n",
    "# create a confusion matrix using the train data\n",
    "CM_train = confusion_matrix(y_train, log.predict(X_train))\n",
    "# get the accuracy score using the train data\n",
    "accuracy = accuracy_score(y_train, log.predict(X_train))\n",
    "# get the error using the train data\n",
    "error = 1 - accuracy \n",
    "# get the precision using the train data\n",
    "precision = precision_score(y_train, log.predict(X_train), average = 'macro')\n",
    "# get the recall using the train data\n",
    "recall = recall_score(y_train, log.predict(X_train), average = 'macro')\n",
    "print(\"Training Data: \\n Accuracy: \", accuracy, \"\\n\", \"Error: \", error, \"\\n Precision: \", precision, \"\\n Recall: \", recall, \"\\n\")\n",
    "\n",
    "# get the confusion matrix using the test data\n",
    "CM_test = confusion_matrix(y_test, log.predict(X_test))    \n",
    "# get the accuracy score using the test data                                       \n",
    "accuracy = accuracy_score(y_test, log.predict(X_test))\n",
    "# get the error using the accuracy from the test data\n",
    "error = 1 - accuracy \n",
    "# get the precision using the test data\n",
    "precision = precision_score(y_test, log.predict(X_test), average = 'macro')\n",
    "# get the recall using the test data\n",
    "recall = recall_score(y_test, log.predict(X_test), average = 'macro')\n",
    "print(\"Testing Data: \\n Accuracy: \", accuracy, \"\\n\", \"Error: \", error, \"\\n Precision: \", precision, \"\\n Recall: \", recall, \"\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Star Classification Final - Sanjana Mishra and Gerri Fox.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
